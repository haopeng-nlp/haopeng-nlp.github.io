---
layout: post
date: 2024-10-02 15:59:00-0400
inline: true
related_posts: false
---
Check out <a href="https://arxiv.org/abs/2410.01485" style="text-decoration:none">LongGen</a> and <a href="https://arxiv.org/abs/2407.17678" style="text-decoration:none">S2-Attention</a>, simple and effective architectures that substantially reduce the KV cache overhead of long-context LLMs. We also released an efficient and easy-to-use <a href="https://github.com/linxihui/dkernel" style="text-decoration:none">CUDA kernel library</a> for various types of sparse attention.