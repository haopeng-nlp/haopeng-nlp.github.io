---
layout: post
date: 2024-02-15 15:59:00-0400
inline: true
related_posts: false
---

Pretrained LLMs can be adapted to handle 128K-long context with surprisingly small amount of continual pretraining. Check out our <a href="https://huggingface.co/papers/2402.10171" style="text-decoration:none">new preprint</a>!
